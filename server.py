"""
Gemini AI MCP Server for Cloud Run
FastAPI + MCP integration for Cloud Run compatibility
Version 2.0 - With Image Generation Support (Free Tier)
"""

import asyncio
import base64
import json
import logging
import os
from typing import Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI
from pydantic import BaseModel, Field
import google.generativeai as genai

# MCP imports
from mcp.server.fastmcp import FastMCP

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(
    format="[%(levelname)s] %(asctime)s - %(message)s",
    level=logging.INFO
)

# Gemini API ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    logger.info("âœ… Gemini API configured")
else:
    logger.warning("âš ï¸ GEMINI_API_KEY not set - Gemini tools will not work")

# MCP ì„œë²„ ì´ˆê¸°í™”
mcp = FastMCP(
    name="gemini_mcp",
    instructions="""
    This is a Gemini AI MCP server that provides various AI-powered tools.
    Available tools:
    - gemini_generate: Generate text using Gemini AI
    - gemini_summarize: Summarize text
    - gemini_translate: Translate text between languages
    - gemini_analyze: Analyze and answer questions about text
    - gemini_code_review: Review and improve code
    - gemini_generate_image: Generate images from text prompts (Free tier: 500/day)
    - gemini_edit_image: Edit existing images with instructions
    """
)


# ============================================================
# Pydantic ì…ë ¥ ëª¨ë¸ ì •ì˜
# ============================================================

class GenerateInput(BaseModel):
    """í…ìŠ¤íŠ¸ ìƒì„± ì…ë ¥ ëª¨ë¸"""
    prompt: str = Field(
        ...,
        description="ìƒì„±í•  í…ìŠ¤íŠ¸ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ (ì˜ˆ: 'íŒŒì´ì¬ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ í•¨ìˆ˜ ì‘ì„±í•´ì¤˜')",
        min_length=1,
        max_length=10000
    )
    model: str = Field(
        default="gemini-2.0-flash",
        description="ì‚¬ìš©í•  Gemini ëª¨ë¸ (gemini-2.0-flash, gemini-1.5-pro ë“±)"
    )
    max_tokens: Optional[int] = Field(
        default=2048,
        description="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜",
        ge=1,
        le=8192
    )
    temperature: Optional[float] = Field(
        default=0.7,
        description="ì°½ì˜ì„± ì¡°ì ˆ (0.0=ê²°ì •ì , 1.0=ì°½ì˜ì )",
        ge=0.0,
        le=1.0
    )


class SummarizeInput(BaseModel):
    """í…ìŠ¤íŠ¸ ìš”ì•½ ì…ë ¥ ëª¨ë¸"""
    text: str = Field(
        ...,
        description="ìš”ì•½í•  í…ìŠ¤íŠ¸",
        min_length=10,
        max_length=50000
    )
    style: str = Field(
        default="concise",
        description="ìš”ì•½ ìŠ¤íƒ€ì¼: 'concise' (ê°„ê²°), 'detailed' (ìƒì„¸), 'bullet_points' (ê¸€ë¨¸ë¦¬ ê¸°í˜¸)"
    )
    language: str = Field(
        default="ko",
        description="ì¶œë ¥ ì–¸ì–´ ì½”ë“œ (ko=í•œêµ­ì–´, en=ì˜ì–´, ja=ì¼ë³¸ì–´ ë“±)"
    )


class TranslateInput(BaseModel):
    """ë²ˆì—­ ì…ë ¥ ëª¨ë¸"""
    text: str = Field(
        ...,
        description="ë²ˆì—­í•  í…ìŠ¤íŠ¸",
        min_length=1,
        max_length=10000
    )
    source_language: str = Field(
        default="auto",
        description="ì›ë³¸ ì–¸ì–´ (auto=ìë™ ê°ì§€, ko=í•œêµ­ì–´, en=ì˜ì–´, ja=ì¼ë³¸ì–´ ë“±)"
    )
    target_language: str = Field(
        ...,
        description="ë²ˆì—­í•  ëŒ€ìƒ ì–¸ì–´ (ko=í•œêµ­ì–´, en=ì˜ì–´, ja=ì¼ë³¸ì–´ ë“±)"
    )


class AnalyzeInput(BaseModel):
    """í…ìŠ¤íŠ¸ ë¶„ì„ ì…ë ¥ ëª¨ë¸"""
    text: str = Field(
        ...,
        description="ë¶„ì„í•  í…ìŠ¤íŠ¸ ë˜ëŠ” ë¬¸ì„œ",
        min_length=1,
        max_length=50000
    )
    question: str = Field(
        ...,
        description="í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë¬¼ì–´ë³¼ ì§ˆë¬¸",
        min_length=1,
        max_length=1000
    )


class CodeReviewInput(BaseModel):
    """ì½”ë“œ ë¦¬ë·° ì…ë ¥ ëª¨ë¸"""
    code: str = Field(
        ...,
        description="ë¦¬ë·°í•  ì½”ë“œ",
        min_length=1,
        max_length=20000
    )
    language: str = Field(
        default="auto",
        description="í”„ë¡œê·¸ë˜ë° ì–¸ì–´ (auto=ìë™ ê°ì§€, python, javascript, typescript ë“±)"
    )
    focus: str = Field(
        default="all",
        description="ë¦¬ë·° ì´ˆì : 'all' (ì „ì²´), 'security' (ë³´ì•ˆ), 'performance' (ì„±ëŠ¥), 'readability' (ê°€ë…ì„±)"
    )


class ImageGenerateInput(BaseModel):
    """ì´ë¯¸ì§€ ìƒì„± ì…ë ¥ ëª¨ë¸"""
    prompt: str = Field(
        ...,
        description="ìƒì„±í•  ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… (ì˜ˆ: 'ìš°ì£¼ë³µì„ ì…ì€ ê³ ì–‘ì´', 'a sunset over mountains')",
        min_length=1,
        max_length=2000
    )
    style: str = Field(
        default="auto",
        description="ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼: 'auto', 'photo' (ì‚¬ì§„), 'illustration' (ì¼ëŸ¬ìŠ¤íŠ¸), 'anime' (ì• ë‹ˆë©”), 'painting' (ê·¸ë¦¼), '3d' (3D ë Œë”ë§)"
    )
    quality: str = Field(
        default="standard",
        description="ì´ë¯¸ì§€ í’ˆì§ˆ: 'standard' (í‘œì¤€), 'hd' (ê³ í’ˆì§ˆ)"
    )


class ImageEditInput(BaseModel):
    """ì´ë¯¸ì§€ í¸ì§‘ ì…ë ¥ ëª¨ë¸"""
    image_base64: str = Field(
        ...,
        description="í¸ì§‘í•  ì´ë¯¸ì§€ì˜ Base64 ì¸ì½”ë”© ë°ì´í„° (PNG/JPEG)"
    )
    instruction: str = Field(
        ...,
        description="í¸ì§‘ ì§€ì‹œì‚¬í•­ (ì˜ˆ: 'ë°°ê²½ì„ íŒŒë€ìƒ‰ìœ¼ë¡œ ë³€ê²½', 'ì•ˆê²½ ì œê±°')",
        min_length=1,
        max_length=1000
    )


# ============================================================
# Gemini API í—¬í¼ í•¨ìˆ˜
# ============================================================

def get_gemini_model(model_name: str = "gemini-2.0-flash"):
    """Gemini ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return genai.GenerativeModel(model_name)


async def generate_content(
    prompt: str,
    model_name: str = "gemini-2.0-flash",
    max_tokens: int = 2048,
    temperature: float = 0.7
) -> str:
    """Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸  ìƒì„±"""
    try:
        model = get_gemini_model(model_name)
        
        generation_config = genai.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=temperature
        )
        
        response = await asyncio.to_thread(
            model.generate_content,
            prompt,
            generation_config=generation_config
        )
        
        return response.text
    except Exception as e:
        logger.error(f"Gemini API ì˜¤ë¥˜: {e}")
        raise


async def generate_image(
    prompt: str,
    style: str = "auto",
    quality: str = "standard"
) -> dict:
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ìƒì„± (ë¬´ë£Œ í‹°ì–´)
    ëª¨ë¸: gemini-2.0-flash-exp (ì´ë¯¸ì§€ ìƒì„± ì§€ì›)
    """
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ë§¤í•‘
    style_prompts = {
        "photo": "A high-quality photograph of",
        "illustration": "A digital illustration of",
        "anime": "An anime-style illustration of",
        "painting": "An oil painting of",
        "3d": "A 3D rendered image of",
        "auto": ""
    }
    
    style_prefix = style_prompts.get(style, "")
    
    # í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    quality_suffix = ", highly detailed, 4K resolution" if quality == "hd" else ""
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    if style_prefix:
        full_prompt = f"{style_prefix} {prompt}{quality_suffix}"
    else:
        full_prompt = f"{prompt}{quality_suffix}"
    
    try:
        # ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ëª¨ë¸ (ë¬´ë£Œ í‹°ì–´ ì§€ì›)
        model = genai.GenerativeModel("gemini-2.0-flash-exp")
        
        # ì´ë¯¸ì§€ ìƒì„± ìš”ì²­
        response = await asyncio.to_thread(
            model.generate_content,
            [f"Generate an image: {full_prompt}"],
            generation_config=genai.GenerationConfig(
                response_mime_type="text/plain"
            )
        )
        
        result = {
            "success": False,
            "image_base64": None,
            "mime_type": None,
            "text_response": None,
            "prompt_used": full_prompt,
            "message": ""
        }
        
        # ì‘ë‹µ ì²˜ë¦¬
        if response.candidates:
            for part in response.candidates[0].content.parts:
                # í…ìŠ¤íŠ¸ ì‘ë‹µ
                if hasattr(part, 'text') and part.text:
                    result["text_response"] = part.text
                # ì´ë¯¸ì§€ ì‘ë‹µ
                if hasattr(part, 'inline_data') and part.inline_data:
                    image_data = part.inline_data
                    result["image_base64"] = base64.b64encode(image_data.data).decode('utf-8')
                    result["mime_type"] = image_data.mime_type
                    result["success"] = True
        
        if result["success"]:
            result["message"] = "ì´ë¯¸ì§€ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            # ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° í…ìŠ¤íŠ¸ ì‘ë‹µ ë°˜í™˜
            result["message"] = "ì´ë¯¸ì§€ ìƒì„±ì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” í”„ë¡¬í”„íŠ¸ì´ê±°ë‚˜, í…ìŠ¤íŠ¸ ì‘ë‹µë§Œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
            if result["text_response"]:
                result["message"] += f" ì‘ë‹µ: {result['text_response'][:200]}"
        
        return result
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "image_base64": None,
            "mime_type": None,
            "text_response": None,
            "prompt_used": full_prompt,
            "message": f"ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜: {str(e)}"
        }


async def edit_image(
    image_base64: str,
    instruction: str
) -> dict:
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ í¸ì§‘
    """
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # Base64 ë””ì½”ë”©
        image_data = base64.b64decode(image_base64)
        
        # ì´ë¯¸ì§€ MIME íƒ€ì… ê°ì§€ (ê°„ë‹¨í•œ ë§¤ì§ ë°”ì´íŠ¸ í™•ì¸)
        if image_data[:8] == b'\x89PNG\r\n\x1a\n':
            mime_type = "image/png"
        elif image_data[:2] == b'\xff\xd8':
            mime_type = "image/jpeg"
        else:
            mime_type = "image/png"  # ê¸°ë³¸ê°’
        
        # ëª¨ë¸ ì¤€ë¹„
        model = genai.GenerativeModel("gemini-2.0-flash-exp")
        
        # ì´ë¯¸ì§€ì™€ ì§€ì‹œì‚¬í•­ì„ í•¨ê»˜ ì „ì†¡
        response = await asyncio.to_thread(
            model.generate_content,
            [
                {
                    "mime_type": mime_type,
                    "data": image_data
                },
                f"Edit this image: {instruction}. Return the edited image."
            ]
        )
        
        result = {
            "success": False,
            "image_base64": None,
            "mime_type": None,
            "text_response": None,
            "instruction": instruction,
            "message": ""
        }
        
        # ì‘ë‹µ ì²˜ë¦¬
        if response.candidates:
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text') and part.text:
                    result["text_response"] = part.text
                if hasattr(part, 'inline_data') and part.inline_data:
                    result["image_base64"] = base64.b64encode(part.inline_data.data).decode('utf-8')
                    result["mime_type"] = part.inline_data.mime_type
                    result["success"] = True
        
        if result["success"]:
            result["message"] = "ì´ë¯¸ì§€ê°€ ì„±ê³µì ìœ¼ë¡œ í¸ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            result["message"] = "ì´ë¯¸ì§€ í¸ì§‘ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            if result["text_response"]:
                result["message"] += f" ì‘ë‹µ: {result['text_response'][:200]}"
        
        return result
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ í¸ì§‘ ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "image_base64": None,
            "mime_type": None,
            "text_response": None,
            "instruction": instruction,
            "message": f"ì´ë¯¸ì§€ í¸ì§‘ ì˜¤ë¥˜: {str(e)}"
        }


# ============================================================
# MCP ë„êµ¬ ì •ì˜ - í…ìŠ¤íŠ¸ ë„êµ¬
# ============================================================

@mcp.tool(
    name="gemini_generate",
    annotations={
        "title": "Gemini í…ìŠ¤íŠ¸ ìƒì„±",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": False,
        "openWorldHint": True
    }
)
async def gemini_generate(params: GenerateInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ë‹¤ì–‘í•œ ìš©ë„ë¡œ ì‚¬ìš© ê°€ëŠ¥:
    - ì°½ì‘ ê¸€ì“°ê¸°
    - ì½”ë“œ ìƒì„±
    - ì•„ì´ë””ì–´ ë¸Œë ˆì¸ìŠ¤í† ë°
    - ì§ˆë¬¸ ë‹µë³€
    """
    logger.info(f">>> ğŸ› ï¸ Tool: 'gemini_generate' called with prompt length: {len(params.prompt)}")
    
    result = await generate_content(
        prompt=params.prompt,
        model_name=params.model,
        max_tokens=params.max_tokens,
        temperature=params.temperature
    )
    
    return result


@mcp.tool(
    name="gemini_summarize",
    annotations={
        "title": "í…ìŠ¤íŠ¸ ìš”ì•½",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
async def gemini_summarize(params: SummarizeInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    """
    logger.info(f">>> ğŸ› ï¸ Tool: 'gemini_summarize' called with text length: {len(params.text)}")
    
    style_prompts = {
        "concise": "ê°„ê²°í•˜ê²Œ í•µì‹¬ë§Œ",
        "detailed": "ìƒì„¸í•˜ê²Œ",
        "bullet_points": "ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬"
    }
    
    language_names = {
        "ko": "í•œêµ­ì–´",
        "en": "ì˜ì–´",
        "ja": "ì¼ë³¸ì–´",
        "zh": "ì¤‘êµ­ì–´"
    }
    
    style_desc = style_prompts.get(params.style, "ê°„ê²°í•˜ê²Œ")
    lang_name = language_names.get(params.language, params.language)
    
    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {style_desc} {lang_name}ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

---
{params.text}
---

ìš”ì•½:"""
    
    result = await generate_content(prompt=prompt, temperature=0.3)
    return result


@mcp.tool(
    name="gemini_translate",
    annotations={
        "title": "í…ìŠ¤íŠ¸ ë²ˆì—­",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
async def gemini_translate(params: TranslateInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.
    """
    logger.info(f">>> ğŸ› ï¸ Tool: 'gemini_translate' called: {params.source_language} -> {params.target_language}")
    
    language_names = {
        "ko": "í•œêµ­ì–´",
        "en": "ì˜ì–´",
        "ja": "ì¼ë³¸ì–´",
        "zh": "ì¤‘êµ­ì–´",
        "es": "ìŠ¤í˜ì¸ì–´",
        "fr": "í”„ë‘ìŠ¤ì–´",
        "de": "ë…ì¼ì–´",
        "auto": "ìë™ ê°ì§€ëœ ì–¸ì–´"
    }
    
    source_name = language_names.get(params.source_language, params.source_language)
    target_name = language_names.get(params.target_language, params.target_language)
    
    if params.source_language == "auto":
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_name}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ì›ë¬¸ì˜ ë‰˜ì•™ìŠ¤ì™€ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.

ì›ë¬¸:
{params.text}

{target_name} ë²ˆì—­:"""
    else:
        prompt = f"""ë‹¤ìŒ {source_name} í…ìŠ¤íŠ¸ë¥¼ {target_name}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ì›ë¬¸ì˜ ë‰˜ì•™ìŠ¤ì™€ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.

ì›ë¬¸:
{params.text}

{target_name} ë²ˆì—­:"""
    
    result = await generate_content(prompt=prompt, temperature=0.2)
    return result


@mcp.tool(
    name="gemini_analyze",
    annotations={
        "title": "í…ìŠ¤íŠ¸ ë¶„ì„ ë° ì§ˆë¬¸ ë‹µë³€",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
async def gemini_analyze(params: AnalyzeInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    """
    logger.info(f">>> ğŸ› ï¸ Tool: 'gemini_analyze' called with question: {params.question[:50]}...")
    
    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
---
{params.text}
---

ì§ˆë¬¸: {params.question}

ë‹µë³€:"""
    
    result = await generate_content(prompt=prompt, temperature=0.3)
    return result


@mcp.tool(
    name="gemini_code_review",
    annotations={
        "title": "ì½”ë“œ ë¦¬ë·°",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
async def gemini_code_review(params: CodeReviewInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë¥¼ ë¦¬ë·°í•©ë‹ˆë‹¤.
    """
    logger.info(f">>> ğŸ› ï¸ Tool: 'gemini_code_review' called with focus: {params.focus}")
    
    focus_prompts = {
        "all": "ì „ë°˜ì ì¸ í’ˆì§ˆ, ë²„ê·¸, ë³´ì•ˆ, ì„±ëŠ¥, ê°€ë…ì„±",
        "security": "ë³´ì•ˆ ì·¨ì•½ì ê³¼ ì ì¬ì  ë³´ì•ˆ ë¬¸ì œ",
        "performance": "ì„±ëŠ¥ ìµœì í™” ê°€ëŠ¥ì„±ê³¼ ë³‘ëª© í˜„ìƒ",
        "readability": "ì½”ë“œ ê°€ë…ì„±, ëª…ëª… ê·œì¹™, êµ¬ì¡°"
    }
    
    focus_desc = focus_prompts.get(params.focus, focus_prompts["all"])
    lang_hint = f"({params.language})" if params.language != "auto" else ""
    
    prompt = f"""ë‹¤ìŒ ì½”ë“œ{lang_hint}ë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”. íŠ¹íˆ {focus_desc}ì— ì´ˆì ì„ ë§ì¶°ì£¼ì„¸ìš”.

```
{params.code}
```

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¦¬ë·°í•´ì£¼ì„¸ìš”:

## ìš”ì•½
(ì½”ë“œì˜ ì „ë°˜ì ì¸ í‰ê°€)

## ë°œê²¬ëœ ë¬¸ì œì 
(ë¬¸ì œì ê³¼ ê°œì„  ì œì•ˆ)

## ê°œì„ ëœ ì½”ë“œ (í•„ìš”ì‹œ)
(ìˆ˜ì •ëœ ì½”ë“œ ì˜ˆì‹œ)

## ì¶”ê°€ ê¶Œì¥ì‚¬í•­
(ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ë“±)
"""
    
    result = await generate_content(prompt=prompt, temperature=0.3, max_tokens=4096)
    return result


# ============================================================
# MCP ë„êµ¬ ì •ì˜ - ì´ë¯¸ì§€ ë„êµ¬ (NEW)
# ============================================================

@mcp.tool(
    name="gemini_generate_image",
    annotations={
        "title": "ì´ë¯¸ì§€ ìƒì„± (ë¬´ë£Œ í‹°ì–´)",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": False,
        "openWorldHint": True
    }
)
async def gemini_generate_image(params: ImageGenerateInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ë¬´ë£Œ í‹°ì–´ì—ì„œ í•˜ë£¨ ì•½ 500ì¥ê¹Œì§€ ìƒì„± ê°€ëŠ¥í•©ë‹ˆë‹¤.
    
    Args:
        params: ImageGenerateInput - ì´ë¯¸ì§€ ìƒì„± ì„¤ì •
            - prompt: ìƒì„±í•  ì´ë¯¸ì§€ ì„¤ëª… (ì˜ˆ: 'ìš°ì£¼ë³µì„ ì…ì€ ê³ ì–‘ì´')
            - style: ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ (auto/photo/illustration/anime/painting/3d)
            - quality: ì´ë¯¸ì§€ í’ˆì§ˆ (standard/hd)
    
    Returns:
        JSON í˜•ì‹ì˜ ê²°ê³¼:
        - success: ì„±ê³µ ì—¬ë¶€
        - image_base64: Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°
        - mime_type: ì´ë¯¸ì§€ MIME íƒ€ì…
        - message: ê²°ê³¼ ë©”ì‹œì§€
        
    Note:
        ë°˜í™˜ëœ image_base64ëŠ” HTMLì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©:
        <img src="data:{mime_type};base64,{image_base64}">
    """
    logger.info(f">>> ğŸ–¼ï¸ Tool: 'gemini_generate_image' called with prompt: {params.prompt[:50]}...")
    
    result = await generate_image(
        prompt=params.prompt,
        style=params.style,
        quality=params.quality
    )
    
    return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool(
    name="gemini_edit_image",
    annotations={
        "title": "ì´ë¯¸ì§€ í¸ì§‘",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": False,
        "openWorldHint": True
    }
)
async def gemini_edit_image(params: ImageEditInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ í¸ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        params: ImageEditInput - ì´ë¯¸ì§€ í¸ì§‘ ì„¤ì •
            - image_base64: í¸ì§‘í•  ì´ë¯¸ì§€ì˜ Base64 ë°ì´í„°
            - instruction: í¸ì§‘ ì§€ì‹œì‚¬í•­ (ì˜ˆ: 'ë°°ê²½ì„ íŒŒë€ìƒ‰ìœ¼ë¡œ ë³€ê²½')
    
    Returns:
        JSON í˜•ì‹ì˜ ê²°ê³¼:
        - success: ì„±ê³µ ì—¬ë¶€
        - image_base64: Base64 ì¸ì½”ë”©ëœ í¸ì§‘ëœ ì´ë¯¸ì§€
        - mime_type: ì´ë¯¸ì§€ MIME íƒ€ì…
        - message: ê²°ê³¼ ë©”ì‹œì§€
    """
    logger.info(f">>> ğŸ–¼ï¸ Tool: 'gemini_edit_image' called with instruction: {params.instruction[:50]}...")
    
    result = await edit_image(
        image_base64=params.image_base64,
        instruction=params.instruction
    )
    
    return json.dumps(result, ensure_ascii=False, indent=2)


# ============================================================
# í—¬ìŠ¤ ì²´í¬ìš© ë¦¬ì†ŒìŠ¤
# ============================================================

@mcp.resource("health://status")
def health_status() -> str:
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return "OK - Gemini MCP Server is running (v2.0 with Image Generation)"


# ============================================================
# FastAPI ì•± ì„¤ì •
# ============================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan for MCP setup"""
    logger.info("ğŸš€ Starting Gemini MCP Server v2.0 with Image Generation...")
    yield
    logger.info("ğŸ‘‹ Shutting down Gemini MCP Server...")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Gemini MCP Server",
    description="Gemini AI MCP Server for Cloud Run with Image Generation",
    version="2.0.0",
    lifespan=lifespan
)


# Health check ì—”ë“œí¬ì¸íŠ¸ (Cloud Runìš©)
@app.get("/")
async def root():
    """Root health check endpoint"""
    return {
        "status": "ok",
        "service": "gemini-mcp-server",
        "version": "2.0.0",
        "features": ["text-generation", "image-generation", "image-editing"]
    }


@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "gemini_configured": GEMINI_API_KEY is not None,
        "version": "2.0.0",
        "tools": [
            "gemini_generate",
            "gemini_summarize", 
            "gemini_translate",
            "gemini_analyze",
            "gemini_code_review",
            "gemini_generate_image",
            "gemini_edit_image"
        ]
    }


# MCP SSE ì•± ë§ˆìš´íŠ¸
app.mount("/mcp", mcp.sse_app())


# ============================================================
# ì„œë²„ ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8080))
    logger.info(f"ğŸš€ Starting server on port {port}")
    
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=port,
        log_level="info"
    )

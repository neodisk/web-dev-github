"""
Gemini AI MCP Server for Cloud Run
FastAPI + MCP integration for Cloud Run compatibility
"""

import asyncio
import logging
import os
from typing import Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI
from pydantic import BaseModel, Field
import google.generativeai as genai

# MCP imports
from mcp.server.fastmcp import FastMCP

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(
    format="[%(levelname)s] %(asctime)s - %(message)s",
    level=logging.INFO
)

# Gemini API ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    logger.info("âœ… Gemini API configured")
else:
    logger.warning("âš ï¸ GEMINI_API_KEY not set - Gemini tools will not work")

# MCP ì„œë²„ ì´ˆê¸°í™”
mcp = FastMCP(
    name="gemini_mcp",
    instructions="""
    This is a Gemini AI MCP server that provides various AI-powered tools.
    Available tools:
    - gemini_generate: Generate text using Gemini AI
    - gemini_summarize: Summarize text
    - gemini_translate: Translate text between languages
    - gemini_analyze: Analyze and answer questions about text
    - gemini_code_review: Review and improve code
    """
)


# ============================================================
# Pydantic ì…ë ¥ ëª¨ë¸ ì •ì˜
# ============================================================

class GenerateInput(BaseModel):
    """í…ìŠ¤íŠ¸ ìƒì„± ì…ë ¥ ëª¨ë¸"""
    prompt: str = Field(
        ...,
        description="ìƒì„±í•  í…ìŠ¤íŠ¸ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ (ì˜ˆ: 'íŒŒì´ì¬ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ í•¨ìˆ˜ ì‘ì„±í•´ì¤˜')",
        min_length=1,
        max_length=10000
    )
    model: str = Field(
        default="gemini-2.0-flash",
        description="ì‚¬ìš©í•  Gemini ëª¨ë¸ (gemini-2.0-flash, gemini-1.5-pro ë“±)"
    )
    max_tokens: Optional[int] = Field(
        default=2048,
        description="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜",
        ge=1,
        le=8192
    )
    temperature: Optional[float] = Field(
        default=0.7,
        description="ì°½ì˜ì„± ì¡°ì ˆ (0.0=ê²°ì •ì , 1.0=ì°½ì˜ì )",
        ge=0.0,
        le=1.0
    )


class SummarizeInput(BaseModel):
    """í…ìŠ¤íŠ¸ ìš”ì•½ ì…ë ¥ ëª¨ë¸"""
    text: str = Field(
        ...,
        description="ìš”ì•½í•  í…ìŠ¤íŠ¸",
        min_length=10,
        max_length=50000
    )
    style: str = Field(
        default="concise",
        description="ìš”ì•½ ìŠ¤íƒ€ì¼: 'concise' (ê°„ê²°), 'detailed' (ìƒì„¸), 'bullet_points' (ê¸€ë¨¸ë¦¬ ê¸°í˜¸)"
    )
    language: str = Field(
        default="ko",
        description="ì¶œë ¥ ì–¸ì–´ ì½”ë“œ (ko=í•œêµ­ì–´, en=ì˜ì–´, ja=ì¼ë³¸ì–´ ë“±)"
    )


class TranslateInput(BaseModel):
    """ë²ˆì—­ ì…ë ¥ ëª¨ë¸"""
    text: str = Field(
        ...,
        description="ë²ˆì—­í•  í…ìŠ¤íŠ¸",
        min_length=1,
        max_length=10000
    )
    source_language: str = Field(
        default="auto",
        description="ì›ë³¸ ì–¸ì–´ (auto=ìë™ ê°ì§€, ko=í•œêµ­ì–´, en=ì˜ì–´, ja=ì¼ë³¸ì–´ ë“±)"
    )
    target_language: str = Field(
        ...,
        description="ë²ˆì—­í•  ëŒ€ìƒ ì–¸ì–´ (ko=í•œêµ­ì–´, en=ì˜ì–´, ja=ì¼ë³¸ì–´ ë“±)"
    )


class AnalyzeInput(BaseModel):
    """í…ìŠ¤íŠ¸ ë¶„ì„ ì…ë ¥ ëª¨ë¸"""
    text: str = Field(
        ...,
        description="ë¶„ì„í•  í…ìŠ¤íŠ¸ ë˜ëŠ” ë¬¸ì„œ",
        min_length=1,
        max_length=50000
    )
    question: str = Field(
        ...,
        description="í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë¬¼ì–´ë³¼ ì§ˆë¬¸",
        min_length=1,
        max_length=1000
    )


class CodeReviewInput(BaseModel):
    """ì½”ë“œ ë¦¬ë·° ì…ë ¥ ëª¨ë¸"""
    code: str = Field(
        ...,
        description="ë¦¬ë·°í•  ì½”ë“œ",
        min_length=1,
        max_length=20000
    )
    language: str = Field(
        default="auto",
        description="í”„ë¡œê·¸ë˜ë° ì–¸ì–´ (auto=ìë™ ê°ì§€, python, javascript, typescript ë“±)"
    )
    focus: str = Field(
        default="all",
        description="ë¦¬ë·° ì´ˆì : 'all' (ì „ì²´), 'security' (ë³´ì•ˆ), 'performance' (ì„±ëŠ¥), 'readability' (ê°€ë…ì„±)"
    )


# ============================================================
# Gemini API í—¬í¼ í•¨ìˆ˜
# ============================================================

def get_gemini_model(model_name: str = "gemini-2.0-flash"):
    """Gemini ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return genai.GenerativeModel(model_name)


async def generate_content(
    prompt: str,
    model_name: str = "gemini-2.0-flash",
    max_tokens: int = 2048,
    temperature: float = 0.7
) -> str:
    """Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸  ìƒì„±"""
    try:
        model = get_gemini_model(model_name)
        
        generation_config = genai.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=temperature
        )
        
        response = await asyncio.to_thread(
            model.generate_content,
            prompt,
            generation_config=generation_config
        )
        
        return response.text
    except Exception as e:
        logger.error(f"Gemini API ì˜¤ë¥˜: {e}")
        raise


# ============================================================
# MCP ë„êµ¬ ì •ì˜
# ============================================================

@mcp.tool(
    name="gemini_generate",
    annotations={
        "title": "Gemini í…ìŠ¤íŠ¸ ìƒì„±",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": False,
        "openWorldHint": True
    }
)
async def gemini_generate(params: GenerateInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ë‹¤ì–‘í•œ ìš©ë„ë¡œ ì‚¬ìš© ê°€ëŠ¥:
    - ì°½ì‘ ê¸€ì“°ê¸°
    - ì½”ë“œ ìƒì„±
    - ì•„ì´ë””ì–´ ë¸Œë ˆì¸ìŠ¤í† ë°
    - ì§ˆë¬¸ ë‹µë³€
    
    Args:
        params: GenerateInput - ìƒì„± ì„¤ì •
            - prompt: ìƒì„±í•  ë‚´ìš©ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸
            - model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: gemini-2.0-flash)
            - max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 2048)
            - temperature: ì°½ì˜ì„± (0.0-1.0, ê¸°ë³¸: 0.7)
    
    Returns:
        str: ìƒì„±ëœ í…ìŠ¤íŠ¸
    """
    logger.info(f">>> ğŸ› ï¸ Tool: 'gemini_generate' called with prompt length: {len(params.prompt)}")
    
    result = await generate_content(
        prompt=params.prompt,
        model_name=params.model,
        max_tokens=params.max_tokens,
        temperature=params.temperature
    )
    
    return result


@mcp.tool(
    name="gemini_summarize",
    annotations={
        "title": "í…ìŠ¤íŠ¸ ìš”ì•½",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
async def gemini_summarize(params: SummarizeInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    
    Args:
        params: SummarizeInput - ìš”ì•½ ì„¤ì •
            - text: ìš”ì•½í•  í…ìŠ¤íŠ¸
            - style: ìš”ì•½ ìŠ¤íƒ€ì¼ (concise/detailed/bullet_points)
            - language: ì¶œë ¥ ì–¸ì–´
    
    Returns:
        str: ìš”ì•½ëœ í…ìŠ¤íŠ¸
    """
    logger.info(f">>> ğŸ› ï¸ Tool: 'gemini_summarize' called with text length: {len(params.text)}")
    
    style_prompts = {
        "concise": "ê°„ê²°í•˜ê²Œ í•µì‹¬ë§Œ",
        "detailed": "ìƒì„¸í•˜ê²Œ",
        "bullet_points": "ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬"
    }
    
    language_names = {
        "ko": "í•œêµ­ì–´",
        "en": "ì˜ì–´",
        "ja": "ì¼ë³¸ì–´",
        "zh": "ì¤‘êµ­ì–´"
    }
    
    style_desc = style_prompts.get(params.style, "ê°„ê²°í•˜ê²Œ")
    lang_name = language_names.get(params.language, params.language)
    
    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {style_desc} {lang_name}ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

---
{params.text}
---

ìš”ì•½:"""
    
    result = await generate_content(prompt=prompt, temperature=0.3)
    return result


@mcp.tool(
    name="gemini_translate",
    annotations={
        "title": "í…ìŠ¤íŠ¸ ë²ˆì—­",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
async def gemini_translate(params: TranslateInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.
    
    Args:
        params: TranslateInput - ë²ˆì—­ ì„¤ì •
            - text: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            - source_language: ì›ë³¸ ì–¸ì–´ (auto=ìë™ ê°ì§€)
            - target_language: ëŒ€ìƒ ì–¸ì–´
    
    Returns:
        str: ë²ˆì—­ëœ í…ìŠ¤íŠ¸
    """
    logger.info(f">>> ğŸ› ï¸ Tool: 'gemini_translate' called: {params.source_language} -> {params.target_language}")
    
    language_names = {
        "ko": "í•œêµ­ì–´",
        "en": "ì˜ì–´",
        "ja": "ì¼ë³¸ì–´",
        "zh": "ì¤‘êµ­ì–´",
        "es": "ìŠ¤í˜ì¸ì–´",
        "fr": "í”„ë‘ìŠ¤ì–´",
        "de": "ë…ì¼ì–´",
        "auto": "ìë™ ê°ì§€ëœ ì–¸ì–´"
    }
    
    source_name = language_names.get(params.source_language, params.source_language)
    target_name = language_names.get(params.target_language, params.target_language)
    
    if params.source_language == "auto":
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_name}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ì›ë¬¸ì˜ ë‰˜ì•™ìŠ¤ì™€ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.

ì›ë¬¸:
{params.text}

{target_name} ë²ˆì—­:"""
    else:
        prompt = f"""ë‹¤ìŒ {source_name} í…ìŠ¤íŠ¸ë¥¼ {target_name}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ì›ë¬¸ì˜ ë‰˜ì•™ìŠ¤ì™€ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.

ì›ë¬¸:
{params.text}

{target_name} ë²ˆì—­:"""
    
    result = await generate_content(prompt=prompt, temperature=0.2)
    return result


@mcp.tool(
    name="gemini_analyze",
    annotations={
        "title": "í…ìŠ¤íŠ¸ ë¶„ì„ ë° ì§ˆë¬¸ ë‹µë³€",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
async def gemini_analyze(params: AnalyzeInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    
    Args:
        params: AnalyzeInput - ë¶„ì„ ì„¤ì •
            - text: ë¶„ì„í•  í…ìŠ¤íŠ¸/ë¬¸ì„œ
            - question: ì§ˆë¬¸
    
    Returns:
        str: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€
    """
    logger.info(f">>> ğŸ› ï¸ Tool: 'gemini_analyze' called with question: {params.question[:50]}...")
    
    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
---
{params.text}
---

ì§ˆë¬¸: {params.question}

ë‹µë³€:"""
    
    result = await generate_content(prompt=prompt, temperature=0.3)
    return result


@mcp.tool(
    name="gemini_code_review",
    annotations={
        "title": "ì½”ë“œ ë¦¬ë·°",
        "readOnlyHint": True,
        "destructiveHint": False,
        "idempotentHint": True,
        "openWorldHint": True
    }
)
async def gemini_code_review(params: CodeReviewInput) -> str:
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë¥¼ ë¦¬ë·°í•©ë‹ˆë‹¤.
    
    Args:
        params: CodeReviewInput - ì½”ë“œ ë¦¬ë·° ì„¤ì •
            - code: ë¦¬ë·°í•  ì½”ë“œ
            - language: í”„ë¡œê·¸ë˜ë° ì–¸ì–´
            - focus: ë¦¬ë·° ì´ˆì  (all/security/performance/readability)
    
    Returns:
        str: ì½”ë“œ ë¦¬ë·° ê²°ê³¼
    """
    logger.info(f">>> ğŸ› ï¸ Tool: 'gemini_code_review' called with focus: {params.focus}")
    
    focus_prompts = {
        "all": "ì „ë°˜ì ì¸ í’ˆì§ˆ, ë²„ê·¸, ë³´ì•ˆ, ì„±ëŠ¥, ê°€ë…ì„±",
        "security": "ë³´ì•ˆ ì·¨ì•½ì ê³¼ ì ì¬ì  ë³´ì•ˆ ë¬¸ì œ",
        "performance": "ì„±ëŠ¥ ìµœì í™” ê°€ëŠ¥ì„±ê³¼ ë³‘ëª© í˜„ìƒ",
        "readability": "ì½”ë“œ ê°€ë…ì„±, ëª…ëª… ê·œì¹™, êµ¬ì¡°"
    }
    
    focus_desc = focus_prompts.get(params.focus, focus_prompts["all"])
    lang_hint = f"({params.language})" if params.language != "auto" else ""
    
    prompt = f"""ë‹¤ìŒ ì½”ë“œ{lang_hint}ë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”. íŠ¹íˆ {focus_desc}ì— ì´ˆì ì„ ë§ì¶°ì£¼ì„¸ìš”.

```
{params.code}
```

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¦¬ë·°í•´ì£¼ì„¸ìš”:

## ìš”ì•½
(ì½”ë“œì˜ ì „ë°˜ì ì¸ í‰ê°€)

## ë°œê²¬ëœ ë¬¸ì œì 
(ë¬¸ì œì ê³¼ ê°œì„  ì œì•ˆ)

## ê°œì„ ëœ ì½”ë“œ (í•„ìš”ì‹œ)
(ìˆ˜ì •ëœ ì½”ë“œ ì˜ˆì‹œ)

## ì¶”ê°€ ê¶Œì¥ì‚¬í•­
(ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ë“±)
"""
    
    result = await generate_content(prompt=prompt, temperature=0.3, max_tokens=4096)
    return result


# ============================================================
# í—¬ìŠ¤ ì²´í¬ìš© ë¦¬ì†ŒìŠ¤
# ============================================================

@mcp.resource("health://status")
def health_status() -> str:
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return "OK - Gemini MCP Server is running"


# ============================================================
# FastAPI ì•± ì„¤ì •
# ============================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan for MCP setup"""
    logger.info("ğŸš€ Starting Gemini MCP Server...")
    yield
    logger.info("ğŸ‘‹ Shutting down Gemini MCP Server...")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Gemini MCP Server",
    description="Gemini AI MCP Server for Cloud Run",
    version="1.0.0",
    lifespan=lifespan
)


# Health check ì—”ë“œí¬ì¸íŠ¸ (Cloud Runìš©)
@app.get("/")
async def root():
    """Root health check endpoint"""
    return {"status": "ok", "service": "gemini-mcp-server"}


@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "gemini_configured": GEMINI_API_KEY is not None
    }


# MCP SSE ì•± ë§ˆìš´íŠ¸
# FastMCPì˜ sse_app() ë©”ì„œë“œë¡œ SSE ì—”ë“œí¬ì¸íŠ¸ ìƒì„±
app.mount("/mcp", mcp.sse_app())


# ============================================================
# ì„œë²„ ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8080))
    logger.info(f"ğŸš€ Starting server on port {port}")
    
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
